# -*- coding: utf-8 -*-
"""csv.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Lbsg6nYZRIMvniXiuryduTX_pqzCN1ra
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
#from tensorflow.keras.layers import *
from keras.utils import to_categorical
import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from sklearn.utils import shuffle
from keras.regularizers import L2
from keras.layers import Dense, Dropout

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv("/content/drive/MyDrive/TRABAJO FINAL/datos_pacientes.csv")
df.head()

temp_target = []
for l in df ['NASAL SIDE']:
  if l == 'L':
    temp_target.append (0)
  else:
    temp_target.append (1)

df ['target'] = np.asarray(temp_target)
print (df ['target'])
print (df ['NASAL SIDE'])

df.dtypes

#Se mezclan aleatoriamente las filas
data_shuffled = shuffle(df)

#Índice para dividir el data frame
split_index = int(0.8 * len(data_shuffled))  # 80% para entrenamiento

# Dividir el DataFrame
d_train = data_shuffled[:split_index]
d_test = data_shuffled[split_index:]



#Obtener los targets correspondientes al conjunto de entrenamiento
target_train = d_train['target']
target_test =  d_test['target']

print(target_train.shape)
print(target_test.shape)

#estadística de cada columna

m_ata_train = d_train['AtA'].values.mean()
s_ata_train = d_train['AtA'].values.std()

m_niW_train = d_train['NASAL IRIS WIDTH'].values.mean()
s_niW_train = d_train['NASAL IRIS WIDTH'].values.std()

m_tiW_train = d_train['TEMPORAL IRIS WIDTH'].values.mean()
s_tiW_train = d_train['TEMPORAL IRIS WIDTH'].values.std()

m_niL_train = d_train['NASAL IRIS LARGE'].values.mean()
s_niL_train = d_train['NASAL IRIS LARGE'].values.std()

m_tiL_train = d_train['TEMPORAL IRIS LARGE'].values.mean()
s_tiL_train = d_train['TEMPORAL IRIS LARGE'].values.std()


m_ata_test = d_test['AtA'].values.mean()
s_ata_test = d_test['AtA'].values.std()

m_niW_test = d_test['NASAL IRIS WIDTH'].values.mean()
s_niW_test = d_test['NASAL IRIS WIDTH'].values.std()

m_tiW_test = d_test['TEMPORAL IRIS WIDTH'].values.mean()
s_tiW_test = d_test['TEMPORAL IRIS WIDTH'].values.std()

m_niL_test = d_test['NASAL IRIS LARGE'].values.mean()
s_niL_test = d_test['NASAL IRIS LARGE'].values.std()

m_tiL_test = d_test['TEMPORAL IRIS LARGE'].values.mean()
s_tiL_test = d_test['TEMPORAL IRIS LARGE'].values.std()

print (m_ata_train)
print (s_ata_train)
print (m_niW_train)
print (s_niW_train)
print (m_tiW_train)
print (s_tiW_train)
print (m_niL_train)
print (s_niL_train)
print (m_tiL_train)
print (s_tiL_train)

#Normalización data

#train
normal_ata_train = (d_train['AtA'].values-m_ata_train)/s_ata_train
normal_niW_train = (d_train['NASAL IRIS WIDTH'].values-m_niW_train)/s_niW_train
normal_tiW_train = (d_train['TEMPORAL IRIS WIDTH'].values-m_tiW_train)/s_tiW_train
normal_niL_train = (d_train['NASAL IRIS LARGE'].values-m_niL_train)/s_niL_train
normal_tiL_train = (d_train['TEMPORAL IRIS LARGE'].values-m_tiL_train)/s_tiL_train

#test
normal_ata_test = (d_test['AtA'].values-m_ata_test)/s_ata_test
normal_niW_test = (d_test['NASAL IRIS WIDTH'].values-m_niW_test)/s_niW_test
normal_tiW_test = (d_test['TEMPORAL IRIS WIDTH'].values-m_tiW_test)/s_tiW_test
normal_niL_test = (d_test['NASAL IRIS LARGE'].values-m_niL_test)/s_niL_test
normal_tiL_test = (d_test['TEMPORAL IRIS LARGE'].values-m_tiL_test)/s_tiL_test

d_train = np.array([d_train['TEMPORAL IRIS LARGE'], d_train['NASAL IRIS LARGE']]).T
d_test = np.array([d_test['TEMPORAL IRIS LARGE'], d_test['NASAL IRIS LARGE']]).T

print(d_train.shape)
print(d_test.shape)


#d_train = np.array([normal_niL_train, normal_tiL_train]).T
#d_test = np.array([normal_niL_test, normal_tiL_test]).T

#d_train = np.array([normal_ata_train, normal_niW_train, normal_tiW_train, normal_niL_train, normal_tiL_train]).T
#d_test = np.array([normal_ata_test, normal_niW_test, normal_tiW_test, normal_niL_test, normal_tiL_test]).T

#target_train = to_categorical(target_train)
print (target_test.shape)

#target_test = to_categorical(target_test)
#print (target_test)

#print (d_train.shape)
#print (d_test.shape)
#print (target_test)


plt.scatter(normal_tiL_train, normal_niL_train)
plt.scatter(df['TEMPORAL IRIS LARGE'],df['NASAL IRIS LARGE'])
plt.grid(True)
plt.show()

#Arquitectura modelo

model = tf.keras.models.Sequential()
model.add(Dense(4, activation='relu', input_dim=2))
#model.add(Dropout(0.5))
model.add(Dense(2, activation='relu'))
#model.add(Dropout(0.25))
model.add(Dense(1, activation='sigmoid'))

#model = tf.keras.models.Sequential()
#model.add(Dense(8, activation='relu', input_dim=5, kernel_regularizer=L2(0.01)))
#model.add(Dropout(0.5))
#model.add(Dense(4, activation='relu', kernel_regularizer=L2(0.01)))
#model.add(Dropout(0.25))
#model.add(Dense(2, activation='sigmoid'))        Accuracy on test data: del 52% aprox con 250 epochs

#model.add(Dense(16, activation='relu', input_dim=5))
#model.add(Dense(8, activation='relu'))
#model.add(keras.layers.Dropout(0.25))
#model.add(Dense(4, activation='relu'))
#model.add(Dense(2, activation='softmax'))



# Compile the model

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])

#Entrenamiento

historial = model.fit(d_train, target_train, epochs=250, shuffle=True , batch_size=60, validation_split=0.15)
model.evaluate(d_test, target_test)
#model.fit(x=d_train, y=target_train, validation_data=(d_test, target_test), epochs=50, batch_size=8)
#SHUFFLE_BUFFER = 500
#BATCH_SIZE = 10

pred_train= model.predict(d_train)
scores = model.evaluate(d_train, target_train, verbose=0)
print('Accuracy on training data: {}% \n Error on training data: {}'.format(scores[1], 1 - scores[1]))
pred_test= model.predict(d_test)
scores2 = model.evaluate(d_test, target_test, verbose=0)

print(f'Accuracy on test data: {scores2[1]}% \n Error on test data: {1 - scores2[1]}')

print(d_train.shape)

model.save("/content/drive/MyDrive/TRABAJO FINAL/LeftRight_model_230718.keras")

#Graficas de precisión
acc = historial.history['accuracy']
val_acc = historial.history['val_accuracy']

loss = historial.history['loss']
val_loss = historial.history['val_loss']

rango_epocas = range(250)

plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.plot(rango_epocas, acc, label='Precisión Entrenamiento')
plt.plot(rango_epocas, val_acc, label='Precisión Pruebas')
plt.legend(loc='lower right')
plt.title('Precisión de entrenamiento y pruebas')

plt.subplot(1,2,2)
plt.plot(rango_epocas, loss, label='Pérdida de entrenamiento')
plt.plot(rango_epocas, val_loss, label='Pérdida de pruebas')
plt.legend(loc='upper right')
plt.title('Pérdida de entrenamiento y pruebas')
plt.show()

